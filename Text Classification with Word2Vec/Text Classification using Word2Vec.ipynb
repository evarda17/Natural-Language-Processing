{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6.1 Text classification with w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same data set Triage as last two weeks. Here is what's in this assignment. \n",
    "\n",
    "1. we will explore text classification with pre-trained w2v embeddings with logistic regression. \n",
    "\n",
    "2. we will explore text classification with w2v embeddings trained on the Triage training dataset and then test it on the dev dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Using pre-trained w2v embeddings for text classification\n",
    "\n",
    "For data loading, you should use the same code as last time so that you can obtain train_text, train_label, dev_text, dev_label, etc. \n",
    "\n",
    "To get pretrained w2v embeddings, we can use the ```gensim``` library. You can do \n",
    "\n",
    "```!pip install gensim```\n",
    "\n",
    "to get it first. \n",
    "\n",
    "One you installed the library, you can take a look at which pretrained embeddings are available for your to download. \n",
    "\n",
    "```\n",
    "import gensim.downloader\n",
    "#Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "```\n",
    "\n",
    "You should see a list of available pretrained embeddings like this: \n",
    "\n",
    "```\n",
    "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
    "```\n",
    "\n",
    "We recommend trying out a few, like the 'glove-wiki-gigaword-300' and 'word2vec-google-news-300'. To download the embeddings: \n",
    "\n",
    "```\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "```\n",
    "\n",
    "Once you downloaded it into a variable, you can do many things. For instance, you can find the most similar words to a query word you put in: \n",
    "\n",
    "```\n",
    "glove_vectors.most_similar('how')\n",
    "```\n",
    "\n",
    "You can also look at the embedding of a word: \n",
    "\n",
    "```\n",
    "word = \"how\"\n",
    "word_embedding = glove_vectors[word]\n",
    "\n",
    "```\n",
    "\n",
    "In tfidf, a sentence or document is naturally represented as a vector by the vocabulary based vectors. However, in w2v, you have a vector for each word, but not a sentence (alternatively, you can use something called doc2vec to directly encode a sentence). The most common way to get a sentence vector from word vectors is just to go through each word, get their embeddings and finally take an average of all word embeddings. If each word is a 300-d vector, then the final sentence vector is also 300-d. \n",
    "\n",
    "### Task 1: Write a ```get_sentence_embedding()``` function. \n",
    "\n",
    "First, you need to write a function to get sentence embeddings from all words. Note that when you look up a word embedding in the pretrained w2v, there is no guarantee that that word is in the w2v dictionary. If not, then you will get an error when you look at that word. In your code, you should build in error handling to take care of this situation. If a word is not present in the dictionary, you should initialize it with a 300-d zero vector using ```numpy.zeros()```. \n",
    "\n",
    "For this task let's use the pre-trained google news 300-d vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all the modules and importing all the requier libraries \n",
    "import gensim.downloader\n",
    "#Show all available models in gensim-data\n",
    "# print(list(gensim.downloader.info()['models'].keys()))\n",
    "import numpy as np\n",
    "\n",
    "from util import load_data, Dataset, Example\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading pretrained embeddings \n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "#tokenizing sentences instead of splitting to achieve higher accuracy:\n",
    "def tokenize(sentence):\n",
    "    return word_tokenize(sentence.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence:str,word2vec_vectors)->np.ndarray:\n",
    "    \"\"\"\n",
    "    function to get embedding of a sentence from the words in it using w2v\n",
    "\n",
    "    args:\n",
    "        sentence: the input sentence to compute embeddings from \n",
    "        glove_vectors: the pretrained w2v object where you can look up word embeddings\n",
    "    returns:\n",
    "        a numpy ndarray with the same dimension as the pretrained w2v embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    #split the sentence into words\n",
    "    words = tokenize(sentence)\n",
    "    embed_dim = 300 \n",
    "\n",
    "    #initializing sentence embedding with zeros of the dimensions of embedding\n",
    "    sentence_embed = np.zeros(embed_dim)\n",
    "    #creating a counter initialized to 0 that we will increment for number of valid words we encounter\n",
    "    num_w = 0\n",
    "\n",
    "    for i in words:\n",
    "        try:\n",
    "            #searching for each word in the glove_vector\n",
    "            word_embed = glove_vectors[i]\n",
    "            #adding each word to the sentence embedding\n",
    "            sentence_embed += word_embed\n",
    "            #incrementing our counter\n",
    "            num_w +=1\n",
    "        #exception in case of an error:\n",
    "        except KeyError: #this is when the word is not found in the dictionary\n",
    "            sentence_embed += np.zeros(embed_dim) #then we will add a zero vector\n",
    "        \n",
    "    if num_w > 0:\n",
    "        sentence_embed = sentence_embed/num_w #finding the average of te word embeddings \n",
    "    return sentence_embed      \n",
    "\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: encode your input sentences from train and test portion of the Triage dataset into vector representations \n",
    "\n",
    "Last week we saw how to use tf-idf vectors to represent sentences and use them in a classifier. Here we just need to similarly turn all training and dev sentences into vectors, but using w2v. \n",
    "\n",
    "Make use of the function above and go through all sentences in your train data and dev data. One possibility is that all of the words in a sentence may be absent from your pretrained w2v dictionary. In that case, it would just come out as a zero vector for the whole sentence, which may not be ideal but let's keep it simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data(\"./data/triage\")\n",
    "\n",
    "def get_data(split: list[Example]) -> (list[str], list[int]):\n",
    "    \"\"\"\n",
    "    Massage the data into a format consistent with the input type required by CountVectorizer or TfidfVectorizer. \n",
    "\n",
    "    Args:\n",
    "        split: pass in the split, which should be either dataset.train or dataset.dev\n",
    "\n",
    "    Returns: \n",
    "        text: list of sentences\n",
    "        labels: list of labels  \n",
    "    \"\"\"\n",
    "    # Extract texts and labels from the Example objects\n",
    "    texts = [\" \".join(example.words) for example in split]  \n",
    "    labels = [example.label for example in split]\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "train_text, train_label = get_data(dataset.train)\n",
    "dev_text, dev_label = get_data(dataset.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the train text sentences\n",
    "train_embed = []\n",
    "for sentence in train_text:\n",
    "    embedding = get_sentence_embedding(sentence, glove_vectors)\n",
    "    train_embed.append(embedding)\n",
    "\n",
    "#encoding the test text sentences\n",
    "test_embed = []\n",
    "for sentence in dev_text:\n",
    "    embedding = get_sentence_embedding(sentence, glove_vectors)\n",
    "    test_embed.append(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Logistic regression text classification with w2v\n",
    "\n",
    "Feed your w2v encoded train data into the logistic regression classifier you worked with last week, except this time you should use the scikit-learn built-in function of logistic regression. Report the accuracy for train and dev datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing before feeding to the log regression -- this helps a lot with accuracy\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "train_embed = normalize(train_embed)\n",
    "test_embed = normalize(test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train prediction accuracy 0.776204504418892\n",
      "test prediction accuracy is: 0.7726389428682472\n"
     ]
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression(random_state=0, max_iter=10000).fit(train_embed, train_label)\n",
    "\n",
    "#predicting using the train dat\n",
    "train_prediction = logistic_reg.predict(train_embed)\n",
    "train_acc = accuracy_score(train_label, train_prediction)\n",
    "print(\"train prediction accuracy\", train_acc)\n",
    "\n",
    "#now running the logistic regression on the test \n",
    "dev_prediction = logistic_reg.predict(test_embed)\n",
    "test_acc = accuracy_score(dev_label, dev_prediction)\n",
    "print(\"test prediction accuracy is:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Train your own w2v embeddimgs on the Triage training data and test it on the dev data\n",
    "\n",
    "In this part we will train our w2v model based on the training dataset. First, you can read through the gensim package tutorial. Pay special attention to the ```training parameters section``` to understand the parameters in the ```Word2Vec``` function below. \n",
    "\n",
    "Assuming you have the ```train_text``` variable set up above, which is a list of sentences, we would still need to break each sentence into a list of words. In the below code, we first do that, then take the three steps to train a w2v model:\n",
    "\n",
    "1. initialize model with ```Word2Vec()```\n",
    "2. build your vocab\n",
    "3. train the model. \n",
    "\n",
    "### Task 3: train w2v model with default parameters\n",
    "\n",
    "using the code below, and then use your above code to feed your text training data and dev data to your logistic regression model with this new trained w2v dictionary. Note that to load the embeddings for a word, you need to look it up by: \n",
    "\n",
    "```word_emb = w2v_vector.wv[word]```\n",
    "\n",
    "Which is a little different from the pre-trained model. \n",
    "\n",
    "After training your logistic regression model, report accuracy for both training and dev data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redoing all the splitting the sentence inso a list of words again\n",
    "dataset = load_data(\"./data/triage\")\n",
    "\n",
    "def get_data(split: list[Example]) -> (list[str], list[int]):\n",
    "    \"\"\"\n",
    "    Massage the data into a format consistent with the input type required by CountVectorizer or TfidfVectorizer. \n",
    "\n",
    "    Args:\n",
    "        split: pass in the split, which should be either dataset.train or dataset.dev\n",
    "\n",
    "    Returns: \n",
    "        text: list of sentences\n",
    "        labels: list of labels  \n",
    "    \"\"\"\n",
    "    # Extract texts and labels from the Example objects\n",
    "    texts = [\" \".join(example.words) for example in split]  \n",
    "    labels = [example.label for example in split]\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "train_text, train_label = get_data(dataset.train)\n",
    "dev_text, dev_label = get_data(dataset.dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: training w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avoid printing all the epochs in log in jupyter notebook:\n",
    "import logging \n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11912516, 16427040)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "W2V_MIN_COUNT = 5\n",
    "W2V_WINDOW = 10 # Or try 5\n",
    "W2V_EPOCH = 32\n",
    "W2V_SIZE = 300\n",
    "\n",
    "\n",
    "sentences = [tokenize(sentence) for sentence in train_text]\n",
    "\n",
    "w2v_model = Word2Vec(vector_size=W2V_SIZE, \n",
    "                    window=W2V_WINDOW, \n",
    "                    min_count=W2V_MIN_COUNT, \n",
    "                    workers=8)\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "w2v_model.train(sentences, total_examples=len(sentences), epochs=W2V_EPOCH, report_delay=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: getting vectors from the train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to vectors\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = tokenize(sentence)\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if vectors:\n",
    "        return normalize(np.mean(vectors, axis=0).reshape(1, -1))[0]\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "#vectors from train text \n",
    "train_w2v = []\n",
    "for sentence in train_text:\n",
    "    embedding = get_sentence_vector(sentence, w2v_model)\n",
    "    train_w2v.append(embedding)\n",
    "    \n",
    "    \n",
    "test_w2v = []\n",
    "for sentence in dev_text:\n",
    "    embedding = get_sentence_vector(sentence, w2v_model)\n",
    "    test_w2v.append(embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data before feeding to logistic regression\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "train_w2v = normalize(train_w2v)\n",
    "test_w2v = normalize(test_w2v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Feeding train/test vectors to the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train prediction accuracy 0.7640881877791504\n",
      "dev prediction accuracy 0.7660318694131364\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "# logistic_reg = LogisticRegression(random_state=0, max_iter=1000) \n",
    "logistic_reg = LogisticRegression(random_state=0, max_iter=10000).fit(train_w2v, train_label)\n",
    "\n",
    "# Predict and report accuracy for training data\n",
    "train_prediction = logistic_reg.predict(train_w2v)\n",
    "train_acc = accuracy_score(train_label, train_prediction)\n",
    "print(\"train prediction accuracy\", train_acc)\n",
    "\n",
    "# Predict and report accuracy for dev data\n",
    "dev_prediction = logistic_reg.predict(test_w2v)\n",
    "dev_acc = accuracy_score(dev_label, dev_prediction)\n",
    "print(\"dev prediction accuracy\", dev_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: play with hyperparameters\n",
    "\n",
    "Change the hyperparameters such as vector_size, window, min_count, etc., and train your w2v model again. Does the accuracy change? Report your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 Chainging Hyperparameters:\n",
    "### experimenting with logistic regression parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters include: C-value of 100 penalty of: l1 resulting in accuracy of 0.6315584920326467 in dev set.\n"
     ]
    }
   ],
   "source": [
    "#tuning values - trying out from a list\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "penalty_val = ['l1', 'l2']\n",
    "\n",
    "best_accuracy = 0\n",
    "best_c_val = None\n",
    "best_penalty = None\n",
    "\n",
    "\n",
    "\n",
    "#iterating through the C-values \n",
    "\n",
    "for c in C_values:\n",
    "    for p in penalty_val:\n",
    "        logistic_reg = LogisticRegression(random_state = 0, \n",
    "                                          max_iter = 10000, \n",
    "                                          C =c, \n",
    "                                          penalty = p, \n",
    "                                          solver = 'liblinear').fit(train_w2v, train_label)\n",
    "        dev_prediction = logistic_reg.predict(test_w2v)\n",
    "        dev_acc = accuracy_score(dev_label, dev_prediction)\n",
    "        \n",
    "        if dev_acc > best_accuracy:\n",
    "            best_accuracy = dev_acc\n",
    "            best_c_val = c\n",
    "            best_penalty = p\n",
    "\n",
    "print(\"best hyperparameters include: C-value of\", best_c_val, \"penalty of:\", best_penalty, \"resulting in accuracy of\", best_accuracy, \"in dev set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with the hyperparameters of the w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "# Define the range of hyperparameters\n",
    "vector_sizes = [350]\n",
    "window_sizes = [5, 8, 10]\n",
    "min_counts = [5, 10, 15]\n",
    "epochs = [20, 30, 40, 100]\n",
    "train_accuracies  = []\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "sentences = [tokenize(sentence) for sentence in train_text]\n",
    "\n",
    "# Iterate through all combinations of hyperparameter values\n",
    "for vector_size in vector_sizes:\n",
    "    for window_size in window_sizes:\n",
    "        for min_count in min_counts:\n",
    "            for epoch in epochs:\n",
    "                # Train Word2Vec model\n",
    "                w2v_model = Word2Vec(vector_size=vector_size, window=window_size, min_count=min_count, workers=8)\n",
    "                w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "                w2v_model.train(sentences, total_examples=len(sentences), epochs=epoch, report_delay=1)\n",
    "                \n",
    "                # Convert text to vectors\n",
    "                # Convert text to vectors and normalize\n",
    "                train_w2v = normalize([get_sentence_vector(sentence, w2v_model) for sentence in train_text])\n",
    "                test_w2v = normalize([get_sentence_vector(sentence, w2v_model) for sentence in dev_text])\n",
    "\n",
    "                \n",
    "                # Train logistic regression model\n",
    "                logistic_reg = LogisticRegression(random_state=0, max_iter=10000).fit(train_w2v, train_label)\n",
    "                \n",
    "                # Evaluate model\n",
    "                dev_prediction = logistic_reg.predict(test_w2v)\n",
    "                dev_acc = accuracy_score(dev_label, dev_prediction)\n",
    "                \n",
    "                # Evaluate model on training set\n",
    "                train_prediction = logistic_reg.predict(train_w2v)\n",
    "                train_acc = accuracy_score(train_label, train_prediction)\n",
    "                \n",
    "                # Update best hyperparameters\n",
    "                if dev_acc > best_acc:\n",
    "                    best_acc = dev_acc\n",
    "                    best_params = (vector_size, window_size, min_count, epoch)\n",
    "                    \n",
    "                # Save train accuracy\n",
    "                train_accuracies.append(train_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "Vector size: 350\n",
      "Window size: 10\n",
      "Min count: 5\n",
      "Epochs: 100\n",
      "Accuracy on dev set: 0.773416245627672\n",
      "Accuracy on train set: 0.7705027083531313\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the best hyperparameter values and the corresponding accuracy\n",
    "print(\"Best hyperparameters:\")\n",
    "print(\"Vector size:\", best_params[0])\n",
    "print(\"Window size:\", best_params[1])\n",
    "print(\"Min count:\", best_params[2])\n",
    "print(\"Epochs:\", best_params[3])\n",
    "print(\"Accuracy on dev set:\", best_acc)\n",
    "print(\"Accuracy on train set:\", max(train_accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the scores from the models and their input vectors from these three weeks. Compare the train and dev accuracy for these configurations: \n",
    "\n",
    "- NB\n",
    "- logistic regression (LR) with unigram count vectors\n",
    "- LR with unigram+bigram count vectors\n",
    "- LR with tfidf vectors\n",
    "- LR with pre-trained w2v vectors\n",
    "- LR with custom trained w2v vectors\n",
    "\n",
    "And analyze the accuracy results from train and dev data. What do you see in terms comparing different methods and input representations? What do you see in terms of train and dev accuracy trends? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_train_acc = 0.8446735721752352\n",
    "nb_dev_acc = 0.7306645938593082\n",
    "\n",
    "lr_tfidf_train_acc = 0.9354271595552599\n",
    "lr_tfidf_dev_acc = 0.757870190439176\n",
    "\n",
    "lr_unigram_train_acc = 0.8003896227311603\n",
    "lr_unigram_dev_acc=  0.7407695297318305\n",
    "\n",
    "\n",
    "lr_unigram_bigram_train_acc=  0.9791884443599734\n",
    "lr_unigram_bigram_dev_acc = 0.7629226583754373\n",
    "\n",
    "lr_pretrained_w2v_train_acc = 0.7616649244512022\n",
    "lr_pretrained_w2v_dev_acc = 0.7586474931986008\n",
    "\n",
    "\n",
    "lr_custom_w2v_train_acc = 0.7705027083531313\n",
    "lr_custom_w2v_dev_acc = 0.773416245627672\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                |   Train Accuracy |   Dev Accuracy |\n",
      "|:---------------------|-----------------:|---------------:|\n",
      "| NB                   |            0.845 |          0.731 |\n",
      "| LR (unigram)         |            0.8   |          0.741 |\n",
      "| LR (unigram+bigram)  |            0.979 |          0.763 |\n",
      "| LR (tfidf)           |            0.935 |          0.758 |\n",
      "| LR (pre-trained w2v) |            0.762 |          0.759 |\n",
      "| LR (custom w2v)      |            0.771 |          0.773 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Your data\n",
    "data = {\n",
    "    \"Model\": [\"NB\", \"LR (unigram)\", \"LR (unigram+bigram)\", \"LR (tfidf)\", \"LR (pre-trained w2v)\", \"LR (custom w2v)\"],\n",
    "    \"Train Accuracy\": [nb_train_acc, lr_unigram_train_acc, lr_unigram_bigram_train_acc, lr_tfidf_train_acc, lr_pretrained_w2v_train_acc, lr_custom_w2v_train_acc],\n",
    "    \"Dev Accuracy\": [nb_dev_acc, lr_unigram_dev_acc, lr_unigram_bigram_dev_acc, lr_tfidf_dev_acc, lr_pretrained_w2v_dev_acc, lr_custom_w2v_dev_acc]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Format columns with accuracies to have 3 digits after the decimal point\n",
    "df[['Train Accuracy', 'Dev Accuracy']] = df[['Train Accuracy', 'Dev Accuracy']].applymap(lambda x: round(x, 3))\n",
    "\n",
    "# Print the dataframe in markdown format\n",
    "print(tabulate(df, headers='keys', tablefmt='pipe', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis & Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaiveBayes:\n",
    "Using this model, we achieve relatively normal accuracy. However, we notice that other models yield higher test accuracies than what we achieve with NaiveBayes.\n",
    "\n",
    "#### LR (unigram):\n",
    "In the case of LR unigram, we observe that train and dev accuracies are close enough, suggesting there is less overfitting. Overall, the training accuracy was higher in NaiveBayes, but the test accuracy was higher with LR unigram compared to NaiveBayes.\n",
    "\n",
    "#### LR (Unigram + Bigram):\n",
    "This model has shown the highest train accuracy so far, but the gap between the train and test accuracies is huge, suggesting overfitting. However, the test accuracy is not low either.\n",
    "\n",
    "#### LR (tfidf):\n",
    "Similar to the previous model, we observe a very high train accuracy rate, but a much lower test accuracy. This suggests overfitting of the model to the train data, with the test accuracy here being lower than that of the unigram and bigram model.\n",
    "\n",
    "#### Pre-trained w2v:\n",
    "While the accuracy for train data here is not the highest (actually, it is the lowest among all), the test and train accuracies are very close in numbers. This suggests that this model is probably among the best ones we have seen so far, as it performs as well with unseen data as it does with train data, meaning the model is good at generalizing.\n",
    "\n",
    "#### LR (custom w2v):\n",
    "While the train accuracy here is not great, we notice something strange - our test accuracy is higher than the train accuracy. The closeness of the accuracies in train and test also suggests good generalization.\n",
    "\n",
    "#### Overall conclusions:\n",
    "We notice certain trends among the models. The models that use word embeddings have closer train and test accuracies than the rest of the models. While the highest train accuracy is achieved with the LR unigram + bigram model, we do not observe the highest test accuracy with this model. Rather, the LR custom w2v model shows a high accuracy for test of 0.773 - the highest observed accuracy so far for test. Thus, to conclude, the best models in terms of generalization and less overfitting are the word-embedding models, especially those using custom w2v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
